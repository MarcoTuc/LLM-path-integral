{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from ./models/Phi-3.5-mini-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Phi-3.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = mini\n",
      "llama_model_loader: - kv   6:                            general.license str              = mit\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\n",
      "llama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"multilingual\"]\n",
      "llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  14:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\n",
      "llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   67 tensors\n",
      "llama_model_loader: - type q8_0:  130 tensors\n",
      "llm_load_vocab: special tokens cache size = 323\n",
      "llm_load_vocab: token to piece cache size = 0.1685 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 3.78 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Phi 3.5 Mini Instruct\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3872.38 MiB\n",
      ".....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   384.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   102.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\", 'phi3.rope.scaling.original_context_length': '4096', 'general.architecture': 'phi3', 'phi3.rope.scaling.attn_factor': '1.190238', 'general.license': 'mit', 'phi3.context_length': '131072', 'general.type': 'model', 'general.license.link': 'https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE', 'tokenizer.ggml.pre': 'default', 'general.basename': 'Phi-3.5', 'tokenizer.ggml.padding_token_id': '32000', 'phi3.attention.head_count': '32', 'phi3.attention.head_count_kv': '32', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.embedding_length': '3072', 'phi3.rope.dimension_count': '96', 'general.finetune': 'instruct', 'general.file_type': '7', 'phi3.rope.freq_base': '10000.000000', 'phi3.attention.sliding_window': '262144', 'phi3.block_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi 3.5 Mini Instruct', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.add_eos_token': 'false'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "import ctypes\n",
    "from pprint import pprint\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"./models/Phi-3.5-mini-instruct.Q8_0.gguf\",\n",
    "    logits_all=True,\n",
    "    n_ctx=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     317.86 ms\n",
      "llama_print_timings:      sample time =       4.93 ms /    12 runs   (    0.41 ms per token,  2436.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    5110.87 ms /    12 runs   (  425.91 ms per token,     2.35 tokens per second)\n",
      "llama_print_timings:       total time =    5129.58 ms /    12 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " I : need a Perl module for handling JSON-RPC requests \n",
      "\n",
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': {'text_offset': [1,\n",
      "                                           6,\n",
      "                                           8,\n",
      "                                           13,\n",
      "                                           20,\n",
      "                                           24,\n",
      "                                           33,\n",
      "                                           38,\n",
      "                                           39,\n",
      "                                           40,\n",
      "                                           42,\n",
      "                                           51],\n",
      "                           'token_logprobs': [-0.052622553,\n",
      "                                              -0.8495209,\n",
      "                                              -2.8424702,\n",
      "                                              -1.470347,\n",
      "                                              -1.0074412,\n",
      "                                              -1.9392164,\n",
      "                                              -4.265588,\n",
      "                                              -1.6175305,\n",
      "                                              -0.6573358,\n",
      "                                              -0.00024399166,\n",
      "                                              -1.4236746,\n",
      "                                              -1.0989814],\n",
      "                           'tokens': [' need',\n",
      "                                      ' a',\n",
      "                                      ' Perl',\n",
      "                                      ' module',\n",
      "                                      ' for',\n",
      "                                      ' handling',\n",
      "                                      ' JSON',\n",
      "                                      '-',\n",
      "                                      'R',\n",
      "                                      'PC',\n",
      "                                      ' requests',\n",
      "                                      '.'],\n",
      "                           'top_logprobs': [{' need': -0.052622553},\n",
      "                                            {' a': -0.8495209},\n",
      "                                            {' Perl': -2.8424702},\n",
      "                                            {' module': -1.470347},\n",
      "                                            {' for': -1.0074412},\n",
      "                                            {' handling': -1.9392164},\n",
      "                                            {' JSON': -4.265588},\n",
      "                                            {'-': -1.6175305},\n",
      "                                            {'R': -0.6573358},\n",
      "                                            {'PC': -0.00024399166},\n",
      "                                            {' requests': -1.4236746},\n",
      "                                            {'.': -1.0989814}]},\n",
      "              'text': ' need a Perl module for handling JSON-RPC requests'}],\n",
      " 'created': 1724754178,\n",
      " 'id': 'cmpl-7fc6a0e9-8e56-4994-a8ed-c354ca463026',\n",
      " 'model': './models/Phi-3.5-mini-instruct.Q8_0.gguf',\n",
      " 'object': 'text_completion',\n",
      " 'usage': {'completion_tokens': 12, 'prompt_tokens': 1, 'total_tokens': 13}}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The pirate was\"\n",
    "\n",
    "output = llm.create_completion(prompt, stop=\".\", logprobs=0, max_tokens=50)\n",
    "text = output[\"choices\"][0][\"text\"]\n",
    "\n",
    "print(\"\\n\", prompt, \":\"+text, \"\\n\")\n",
    "pprint(output)\n",
    "\n",
    "observed_logprob = sum(output[\"choices\"][0][\"logprobs\"][\"token_logprobs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I am marco\"\n",
    "tokens = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "llm.reset()\n",
    "llm.eval(tokens)\n",
    "logits = llm.eval_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_tensor = torch.tensor(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21.5399, 26.9652, 26.0001,  ..., 26.2229, 26.2232, 26.2291],\n",
       "        [36.9913, 42.5345, 42.1284,  ..., 36.4904, 36.4894, 36.4884],\n",
       "        [24.3613, 25.4191, 24.4385,  ..., 21.4203, 21.4199, 21.4277],\n",
       "        [37.2980, 39.1745, 40.5854,  ..., 33.8172, 33.8082, 33.8178]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
